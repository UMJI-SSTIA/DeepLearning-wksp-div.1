# DL workshop 1 - 2nd part

接下来我们具体介绍神经网络最基本的理论知识。

![感知器](img/perceptron.png)

## 1 Perceptron

神经网络由多个层组成，每个层都由许多神经元组成。每个神经元接收一些输入，将这些输入与其权重相乘，然后应用一个激活函数，如ReLU或sigmoid，输出然后传递到下一层。

### 1.1 线性变换

在这个步骤中，神经元接收一些输入$x_1, x_2, ..., x_n$，每个输入都有一个相应的权重$w_1, w_2, ..., w_n$。神经元将每个输入与其相应的权重相乘，然后将所有的结果加起来，得到一个总和。这个过程可以用下面的公式表示：

$$
z = \sum_{i=1}^{n} w_i x_i + w_0
$$

其中$w_0$是偏置项，它是一个常数，可以让神经元更灵活地适应数据。

### 1.2 非线性变换

在这个步骤中，神经元将线性变换的结果$z$通过一个非线性函数，也称为激活函数。这个函数可以是sigmoid、ReLU（Rectified Linear Unit）、tanh（双曲正切）等。激活函数的选择取决于特定的应用和网络架构。激活函数的目的是引入非线性，使神经网络能够学习并表示更复杂的模式。

Note: 在实践中，ReLU是最常用的激活函数，因为它的计算速度快，且它的导数在大部分区域都是1，有助于避免梯度消失问题。

#### 什么是梯度消失问题（以及为什么用ReLU激活函数）

梯度消失问题是指在深度神经网络中，梯度在反向传播过程中逐渐变小，导致底层的权重几乎不会更新。这会导致底层的神经元学习缓慢，甚至不学习。

用人话说就是：训练次数越多，导数越小（显然），所以需要导数为1的激活函数。额……非常简单粗暴。

## 2. 训练神经网络

训练神经网络的目标是找到一组权重和偏置，使得网络的输出尽可能接近训练数据的目标输出。

比较直观的方法便是用一个函数表示网络的输出与目标输出之间的差距，然后找到函数的最小值。这个函数通常被称为损失函数。

故主要步骤如下：

1. 前向传播：计算网络的输出
2. 计算损失：计算网络输出与目标输出之间的差距
3. 反向传播：计算损失相对于每个权重的梯度
4. 更新权重：使用梯度下降更新权重
5. 重复：重复步骤1-4直到网络收敛

### 2.1 损失函数怎么算？

在分类问题中，我们通常使用交叉熵损失函数。在回归问题中，我们通常使用均方误差损失函数。

接下来我们用$y_i$表示目标输出，$p_i$表示网络输出。

#### 2.1.1 交叉熵损失函数

$$
L = -\sum_{i} y_i \log(p_i)
$$

#### 2.1.2 均方误差损失函数

$$
L = \frac{1}{2} \sum_{i} (y_i - p_i)^2
$$

### 2.1 反向传播

反向传播是一种算法，用于在神经网络中更新权重，以便模型可以学习更好的表示。它首先计算网络输出与目标输出之间的误差，然后计算这个误差相对于每个权重的梯度。

#### 如何理解“反向传播”？

我们回到前面的图片。

![感知器](img/perceptron.png)

图片中每个箭头其实可以理解成函数。我们的目标是得出损失函数对每个权重的导数。与其粗暴地，“一步到位”地直接求解，不如从后往前，一步一步地求解，以减少运算的次数。我们从高中就开始学习的链式法则便能很好地解决这个问题。

### 2.2 梯度下降

梯度下降是一种优化算法，用于找到函数的最小值。在神经网络中，这个函数是损失函数，它衡量了网络输出与目标输出之间的差距。通过反复更新权重以减小梯度，网络最终会收敛到一组能够最小化损失的权重。有以下几种梯度下降的方法：

#### 2.2.1 批量梯度下降

在批量梯度下降中，我们计算整个训练集的损失，然后计算这个损失相对于每个权重的梯度。这种方法通常会导致更稳定的收敛，但计算成本更高。

#### 2.2.2 随机梯度下降

在随机梯度下降中，我们计算一个样本的损失，然后计算这个损失相对于每个权重的梯度。这种方法通常会导致更快的收敛，但可能会不稳定。

#### 2.2.3 小批量梯度下降

小批量梯度下降是批量梯度下降和随机梯度下降的折中。在这种方法中，我们计算一个小批量样本的损失，然后计算这个损失相对于每个权重的梯度。这种方法通常会导致较稳定的收敛，计算成本也很低。

## 参考资料

- [Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville](http://www.deeplearningbook.org/)
- [Deep Learning Specialization by Andrew Ng on Coursera](https://www.coursera.org/specializations/deep-learning)
- [MIT Intro to Deep Learning](http://introtodeeplearning.com/)
- [交大密院Deep Learning学习手册](https://github.com/Banyutong/deep_learning_hands_on)
- GPT 4.0 (不觉得很酷吗，很符合我对AI的想像)
