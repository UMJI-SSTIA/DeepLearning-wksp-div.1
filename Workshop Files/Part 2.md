# DL workshop 1 - 2nd part

接下来我们具体介绍神经网络最基本的理论知识。

![感知器](img/perceptron.png)

## 1 Perceptron

神经网络由多个层组成，每个层都由许多神经元组成。每个神经元接收一些输入，将这些输入与其权重相乘，然后应用一个激活函数（非线性），输出然后传递到下一层。

### 1.1 线性变换

在这个步骤中，神经元接收一些输入$x_1, x_2, ..., x_n$，每个输入都有一个相应的权重$w_1, w_2, ..., w_n$。神经元将每个输入与其相应的权重相乘，然后将所有的结果加起来，得到一个总和。这个过程可以用下面的公式表示：

$$
z = \sum_{i=1}^{n} w_i x_i + w_0
$$

其中$w_0$是一个常数，我们把它称为偏置项。

偏置项的主要作用是调整神经元的输出。具体来说，如果没有偏置项，那么当所有的输入都为0时，神经元的输出也将为0。但是，有时我们希望神经元在所有输入都为0时也能有非零的输出，于是便有了偏置项。偏置项的存在使得神经元可以更灵活地适应数据。

### 1.2 非线性变换

在这个步骤中，神经元将线性变换的结果$z$通过一个非线性函数，也称为激活函数。这个函数可以是sigmoid、ReLU（Rectified Linear Unit）、tanh（双曲正切）等。激活函数的选择取决于特定的应用和网络架构。引入非线性能够使神经网络能够学习并表示更复杂的模式。

Note: 在实践中，ReLU是最常用的激活函数，因为它的计算速度快，且它的导数在大部分区域都是1，有助于避免梯度消失问题。

#### 什么是梯度消失问题（以及为什么用ReLU激活函数）

梯度消失问题是指在深度神经网络中，梯度在反向传播过程中逐渐变小，导致底层的权重几乎不会更新。这会导致底层的神经元学习缓慢，甚至不学习。

用人话说就是：训练次数越多，导数越小（显然），所以需要导数为1的激活函数。额……非常简单粗暴。

## 2. 训练神经网络

训练神经网络的目标是找到一组权重和偏置，使得网络的输出尽可能接近训练数据的目标输出。

比较直观的方法便是用一个函数表示网络的输出与目标输出之间的差距，然后找到函数的最小值。这个函数通常被称为损失函数。

故主要步骤如下：

1. 前向传播：计算网络的输出
2. 计算损失：计算网络输出与目标输出之间的差距
3. 反向传播：计算损失相对于每个权重的梯度
4. 更新权重：使用梯度下降更新权重
5. 重复：重复步骤1-4直到网络收敛

### 2.1 损失函数怎么算？

在回归问题中，我们通常使用均方误差损失函数；在分类问题中，我们通常使用交叉熵损失函数。这是因为交叉熵函数对概率分布的差异相对更敏感一些。

#### 2.1.1 均方误差损失函数

$$
L = \frac{1}{n} \sum_{i}^n (y_i - p_i)^2
$$

其中$y_i$表示目标输出（预期结果），$p_i$表示网络输出（实际结果），$n$表示样本数。

#### 2.1.2 交叉熵损失函数

$$
L = -\frac{1}{n} \sum_{i=1}^{n} \sum_{c=1}^m y_{i,c} \log p_{i,c}
$$

其中$m$为分类的类别数，$n$为样本数，$y_{i,c}$表示样本$i$的类别$c$的目标输出，$p_{i,c}$表示样本$i$的类别$c$的网络输出。

当然，还有很多别的分类函数，各位感兴趣可以自行查找。

### 2.1 反向传播

反向传播是一种算法，用于在神经网络中更新权重，以便模型可以学习更好的表示。它首先计算网络输出与目标输出之间的误差，然后计算这个误差相对于每个权重的梯度。

#### 如何理解“反向传播”？

我们回到前面的图片。

![感知器](img/perceptron.png)

图片中的sum, non-linearity本质上是函数，而我们的目标是得出在其基础上损失函数对每个权重的导数。有人可能会说直接“一步到位”地求解，但这只是最简单的情况。

![Single Layer NN](img/Single_Layer_NN.png)

实际上，深度学习的网络复杂度远超想像（感知器只是最小的一个单元）。

所以不如从后往前，通过对每个函数单独求导，一步一步地求解，以减少运算的复杂度与次数。我们从高中就开始学习的链式法则便能很好地解决这个问题。

### 2.2 梯度下降

我们现在有了损失函数，也能算出损失函数对每个权重的导数了。然后呢？

![Loss_Function](img/Loss_Function.png)

记住我们的目标：找到损失函数的最小值点。

于是我们选择梯度下降。

![Gradient_Descent](img/Gradient_Descent.png)

于是，算法如下：

1. 随机初始化权重
2. 计算损失函数对每个权重的导数：$\frac{\partial L}{\partial w_i}$
3. 更新权重：$w_i = w_i - \alpha \frac{\partial L}{\partial w_i}$
4. 重复步骤2-3直到网络收敛

其中$\alpha$是学习率，它决定了我们在每次更新权重时应该改变多少。如果学习率太大，我们可能会错过最小值点；如果学习率太小，我们可能会收敛得很慢。

这时一些对时间复杂度敏感的xpy就发现了，每次循环我们都要算一遍整个数据集的梯度，这样的计算量是不是太大了？

所以我们接下来探究一下不同梯度下降的方法。

#### 2.2.1 批量梯度下降

在批量梯度下降中，我们计算整个训练集的损失，然后计算这个损失相对于每个权重的梯度。这种方法通常会导致更稳定的收敛，但计算成本更高。

#### 2.2.2 随机梯度下降

在随机梯度下降中，我们计算一个样本的损失，然后计算这个损失相对于每个权重的梯度。这种方法通常会导致更快的收敛，但可能会不稳定。

**可以看出，这两种方法都很极端，所以我们需要一个折中的方法。**

#### 2.2.3 小批量梯度下降

小批量梯度下降是批量梯度下降和随机梯度下降的折中。在这种方法中，我们计算一个小批量样本的损失，然后计算这个损失相对于每个权重的梯度。这种方法通常会导致较稳定的收敛，计算成本也很低。

## 参考资料

- [Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville](http://www.deeplearningbook.org/)
- [Deep Learning Specialization by Andrew Ng on Coursera](https://www.coursera.org/specializations/deep-learning)
- [MIT Intro to Deep Learning](http://introtodeeplearning.com/)
- [交大密院Deep Learning学习手册](https://github.com/Banyutong/deep_learning_hands_on)
- GPT 4.0 (不觉得很酷吗，很符合我对AI的想像)
